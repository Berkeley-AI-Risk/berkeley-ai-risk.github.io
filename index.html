<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Berkeley AI Risk</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="header-content">
                <h1 class="logo"><a href="index.html" class="logo-link">Berkeley AI Risk</a></h1>
                <nav class="nav">
                    <a href="speaker-series.html" class="nav-link">Speaker Series</a>
                    <a href="reading-group.html" class="nav-link">Reading Group</a>
                    <a href="#about" class="nav-link">About</a>
                    <a href="#organizers" class="nav-link">Organizers</a>
                </nav>
            </div>
        </div>
    </header>

    <main>
        <section class="hero">
            <div class="container">
                <div class="hero-content">
                    <h2 class="hero-title">Berkeley AI Risk</h2>
                    <p class="hero-subtitle">Interdisciplinary discussion of risks to society posed by artificial intelligence</p>
                    <div class="hero-links">
                        <a href="speaker-series.html" class="hero-btn">Speaker Series</a>
                        <a href="reading-group.html" class="hero-btn secondary">Reading Group</a>
                    </div>
                </div>
            </div>
        </section>

        <section id="about" class="about-section">
            <div class="container">
                <h2 class="section-title">About Berkeley AI Risk</h2>
                <div class="about-content">
                    <p>Berkeley AI Risk is an interdisciplinary community of faculty and students at UC Berkeley concerned about risks to society posed by artificial intelligence and how to mitigate them.</p>
                    
                    <p>Scholars at Berkeley concerned about AI risk come from many units on campus, including:</p>
                    
                    <div class="departments-grid">
                        <div class="department">Agricultural &amp; Resource Economics</div>
                        <div class="department">Ancient Greek &amp; Roman Studies</div>
                        <div class="department">Applied Science and Technology</div>
                        <div class="department">Biostatistics</div>
                        <div class="department">Center for Effective Global Action</div>
                        <div class="department">Center for Science, Technology, Medicine &amp; Society</div>
                        <div class="department">Center for Information Technology Research in the Interest of Society</div>
                        <div class="department">College of Computing, Data Science, and Society</div>
                        <div class="department">Cognitive Science</div>
                        <div class="department">Computational Precision Health</div>
                        <div class="department">Computer Science</div>
                        <div class="department">Criminal Law &amp; Justice Center</div>
                        <div class="department">Data Science</div>
                        <div class="department">Demography</div>
                        <div class="department">Economics</div>
                        <div class="department">Education</div>
                        <div class="department">Electrical Engineering and Computer Science</div>
                        <div class="department">English</div>
                        <div class="department">Environmental Science, Policy and Management</div>
                        <div class="department">Epidemiology and Biostatistics</div>
                        <div class="department">Goldman School of Public Policy</div>
                        <div class="department">History</div>
                        <div class="department">Industrial Engineering &amp; Operations Research</div>
                        <div class="department">Information and Data Science</div>
                        <div class="department">Information Technology</div>
                        <div class="department">Institute for Data Science</div>
                        <div class="department">Interdisciplinary Studies</div>
                        <div class="department">Labor Center</div>
                        <div class="department">Law</div>
                        <div class="department">Logic</div>
                        <div class="department">Management</div>
                        <div class="department">Mathematics</div>
                        <div class="department">Music</div>
                        <div class="department">Neuroscience</div>
                        <div class="department">Philosophy</div>
                        <div class="department">Political Science</div>
                        <div class="department">Public Policy</div>
                        <div class="department">School of Information</div>
                        <div class="department">School of Law</div>
                        <div class="department">Social Science Data Laboratory</div>
                        <div class="department">Sociology</div>
                        <div class="department">Statistics</div>
                        <div class="department">Theater, Dance, and Performance Studies</div>
                    </div>
                    
                    <h3 class="questions-heading">Key Questions</h3>
                    
                    <ul>
                        <li>How do we prevent the gradual disempowerment of humans as more societal roles are turned over to AI systems?</li>
                        <li>How do we avoid concentrations of power and rising inequality if the value of human labor declines as the value of AI capital increases?</li>
                        <li>How might economic, geopolitical, and other selection pressures on AI systems threaten to erode the safeguards we attempt to put in place to ensure these systems are safe?</li>
                        <li>Humans have collectively chosen not to develop some technologies, such as human cloning, and not to proliferate others, such as nuclear weapons; is it possible to do the same concerning autonomous, generally intelligent AI systems?</li>
                    </ul>

                    <div class="about-features">
                        <div class="feature">
                            <h3>Speaker Series</h3>
                            <p>Talks by experts from academia, civil society, and industry on AI risks and mitigations</p>
                        </div>
                        <div class="feature">
                            <h3>Reading Group</h3>
                            <p>Discussions of papers and research on AI risk, safety, ethics, and alignment</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="organizers" class="contact-section">
            <div class="container">
                <h2 class="section-title">Organizers</h2>
                <div class="contact-content">
                    <div class="organizers-grid">
                        <div class="organizer">
                            <div class="contact-info">
                                <div class="contact-item">
                                    <strong><a href="https://www.stat.berkeley.edu/~wfithian/" target="_blank" rel="noopener noreferrer" class="organizer-link">Will Fithian</a></strong> (UCB Statistics)
                                </div>
                                <div class="contact-item">
                                    <a href="mailto:wfithian@berkeley.edu" class="organizer-link">wfithian@berkeley.edu</a>
                                </div>
                            </div>
                        </div>
                        <div class="organizer">
                            <div class="contact-info">
                                <div class="contact-item">
                                    <strong><a href="https://wesholliday.net" target="_blank" rel="noopener noreferrer" class="organizer-link">Wes Holliday</a></strong> (UCB Philosophy and Logic)
                                </div>
                                <div class="contact-item">
                                    <a href="mailto:wesholliday@berkeley.edu" class="organizer-link">wesholliday@berkeley.edu</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="contact-item">
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Berkeley AI Risk</p>
        </div>
    </footer>

</body>
</html>
